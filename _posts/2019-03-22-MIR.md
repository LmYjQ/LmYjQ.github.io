---
layout:     post
title:      MIR
subtitle:   Music Information Retrieval
date:       2019-03-22
author:     LmYjQ
header-img: img/MIR_head.jpg
catalog: true
tags:
    - MIR
    - Deep Learning
---

> 译自 [《A Tutorial on Deep Learning for Music Information Retrieval》](https://arxiv.org/pdf/1709.04396.pdf)

### 目录

- 1.动机（略）
- 2.深度学习基础
- 3.MIR任务介绍
- 4.核心模块
- 5.辅助模块
- 6.总结

### 2. 深度学习基础

> **BP+SGD求解神经网络；CNN提取特征，LSTM针对序列数据；Relu激活函数**

#### 2.1 深度学习和"传统"方法对比
传统： 输入->特征提取（MFCC）->分类器->结果  
深度学习：输入->很多很多层->结果  
用“很多很多层”代替“特征提取（MFCC）+分类器”  

#### 2.2 网络如何设计&训练
设计：网络结构长什么样子，由超参数决定（hyperparameters），训练过程中不变  
损失函数：定义”理想“和”现实“的差距  
训练：将数据喂给定义好网络结构和损失函数的神经网络学习，学习过程中参数（weight）不停的变化，直到收敛  
梯度：学习过程中需要走的路  
学习率：每一步走多大。有一系列动态调节的机制，adam，rmsprop，adagrad，momentum等  
激活函数：每一个神经元输入与输出的关系，产生非线性  

#### 2.3 数据量小怎么办
数据增强（data augmentation）:在不破坏原始数据的前提下增加加噪声，当作新的样本  
迁移学习（transfer learning）:用其他任务训练好的网络当作特征提取器，前提假设是两个任务的数据有相似性  
随机网络:随机初始化的CNN可以达到不错的特征提取效果（惊呆🤯了）  

### 3. MUSIC INFORMATION RETRIEVAL

#### 3.1 MIR任务
不是所有问题都能只靠音频信号解决，分类/标签等信息很有用。  
关注两个维度  
> （label的）主观度: 音高/调性/节奏/和声是客观的；风格，情绪是主观的  
> 时间跨度: 音高/和声只要很短的片段就可以判断；情绪/调性需要比较长的旋律才能判断  

#### 3.2 音频信号表达
二维2D：时间域+频率域  
STFT(short-time fourier transform):  
Mel频谱:  
Constant-Q Transform (CQT):  
Chromagram:  

### 4. 用于MIR的深度网络

#### 4.1 DenseLayer如何定义
二维2D：时间域+频率域  
N Number of channels of a 2D representation声道数  
F Frequency-axis length of a 2D representation频域维度  
T Time-axis length of a 2D representation时域维度  
H Height of a 2D convolution kernel (frequency-axis)频域卷积核维度  
W Width of a 2D convolution kernel (time-axis)时域卷积核维度  
V Number of hidden nodes of a layer隐藏节点数  
L Number of layers of a network网络层数  
每一层有输入和输出维度Vin和Vout

#### 4.2 DenseLayer用于MIR
在2D数据（时域+频域表达）之上使用DenseLayer，对每一帧做非线性变换，还可以利用相邻帧作为上下文信息  
DenseLayer不具有平移/放缩不变性  

#### 4.3 ConvLayer卷积层
$$
y^{j}=f\left(\sum_{k=0}^{K-1} \mathbf{W}^{j k} * x^{k}+b^{j}\right)
$$  
weight sharing共享权重:卷积核在输入样本中“滑动”，减少参数量  
feature map: 卷积核在每一个位置激活得到的输出，空间不变性  
pooling layer: 减少feature map的大小，下采样
stacking: 多个卷积层串联，提取复杂特征
kernel size: 不宜过小，学不到信息。也不应过大，用多个小kernel stacking比用一个大kernel好

#### 4.5 RecurrentLayer循环层
$$
\begin{array}{c}{y_{t}=f_{o u t}\left(\mathbf{V} h_{t}\right)} \\ {h_{t}=f_{h}\left(\mathbf{U} x_{t}+\mathbf{W} h_{t-1}\right)}\end{array}
$$  
和HMM类似，输入状态x和之前的隐状态生成新的隐状态，隐状态激活得到最终输出  
实际使用中，因为RNN存在梯度消失/爆炸问题，通过加入遗忘机制的GRU/LSTM解决  

#### 4.6 RecurrentLayer用于MIR
输入和隐层全联接，也可以用全联接层代替  
RecurrentLayer内不能使用平移不变性，RecurrentLayer适用于序列特征（为啥？），比如MFCC或卷积层输出的feature map  
输出层的维度V要足够大才能保留足够多的信息  
考虑性能：控制RecurrentLayer长度  
不同的问题考虑用不同的结构: many-to-many, many-to-one  
未来的信息有用，可以用bi-directional  

### 5. 实践经验




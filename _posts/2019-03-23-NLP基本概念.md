---
layout:     post
title:      NLP基本概念
subtitle:   NLP基本概念
date:       2019-03-23
author:     LmYjQ
header-img: img/post-bg-BJJ.jpg
catalog: true
tags:
    - NLP
    -Deep Learning
---


## 目录

- NNLM
- Word2vec
- ELMO
- attention
- Transformer
- GPT
- BERT

## ELMO
可以根据上下文调整embedding，解决多义词问题  
网络结构：双层双向LSTM  
从前向后：单词特征，句法特征，语义特征

## attention
以翻译为例，Encoder-Decoder框架：输入--编码--输出。如果输入的每一个词对输出的每一个词贡献没有区别，不太合理  
注意力机制体现了每一个输出单词重要性的**分布**  
用一个F(hj,Hi-1)函数，即第j个源单词和第i-1个输出隐状态去评估注意力分布

soft attention: 多个输入重要性分布对一个输出的影响  
self attention: 输入自己对自己的重要性分布

## Transformer
encoder中加入self attention，decoder中依然用soft attention  

## GPT

## BERT
 

